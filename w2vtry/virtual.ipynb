{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# model imports\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set()  # defines the style of the plots to be seaborn style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3798, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv')\n",
    "\n",
    "print(train_data.shape)\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName           0\n",
       "ScreenName         0\n",
       "Location         834\n",
       "TweetAt            0\n",
       "OriginalTweet      0\n",
       "Sentiment          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there's missing data\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words in a document is: 32.6216429699842.\n",
      "The minimum number of words in a document is: 2.\n",
      "The maximum number of words in a document is: 79.\n"
     ]
    }
   ],
   "source": [
    "document_lengths = np.array(list(map(len, train_data.OriginalTweet\n",
    ".str.split(' '))))\n",
    "\n",
    "print(\"The average number of words in a document is: {}.\".format(np.mean(document_lengths)))\n",
    "print(\"The minimum number of words in a document is: {}.\".format(min(document_lengths)))\n",
    "print(\"The maximum number of words in a document is: {}.\".format(max(document_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "ax.set_title(\"Distribution of number of words\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of words\")\n",
    "sns.distplot(document_lengths, bins=50, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 documents with over 150 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} documents with over 150 words.\".format(sum(document_lengths > 150)))\n",
    "\n",
    "shorter_documents = document_lengths[document_lengths <= 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 1551 words with characters with an ordinal >= 128 in the train data.\n"
     ]
    }
   ],
   "source": [
    "# find and remove non-ascii words\n",
    "# I stored our special word in a variable for later use\n",
    "our_special_word = 'https'\n",
    "\n",
    "def remove_ascii_words(df):\n",
    "    \"\"\" removes non-ascii characters from the 'texts' column in df.\n",
    "    It returns the words containig non-ascii characers.\n",
    "    \"\"\"\n",
    "    non_ascii_words = []\n",
    "    for i in range(len(df)):\n",
    "        for word in df.loc[i, 'OriginalTweet'].split(' '):\n",
    "            if any([ord(character) >= 128 for character in word]):\n",
    "                non_ascii_words.append(word)\n",
    "                df.loc[i, 'text'] = df.loc[i, 'OriginalTweet'].replace(word, our_special_word)\n",
    "    return non_ascii_words\n",
    "\n",
    "non_ascii_words = remove_ascii_words(train_data)\n",
    "\n",
    "print(\"Replaced {} words with characters with an ordinal >= 128 in the train data.\".format(\n",
    "    len(non_ascii_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_good_tokens(OriginalTweet):\n",
    "    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), OriginalTweet))\n",
    "    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n",
    "    return removed_punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7a8b9a1ff495>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mw2v_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-7a8b9a1ff495>\u001b[0m in \u001b[0;36mw2v_preprocessing\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#df['OriginalTweet'] = df.OriginalTweet.str.lower()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#df['OriginalTweet'] = df.OriginalTweet.str.split('.')  # split texts into individual sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     df['tokenized_OriginalTweet'] = list(map(lambda OriginalTweet:\n\u001b[0m\u001b[0;32m      9\u001b[0m                                          list(map(nltk.word_tokenize, OriginalTweet)),df.OriginalTweet))# tokenize sentences\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-7a8b9a1ff495>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(OriginalTweet)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#df['OriginalTweet'] = df.OriginalTweet.str.split('.')  # split texts into individual sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     df['tokenized_OriginalTweet'] = list(map(lambda OriginalTweet:\n\u001b[1;32m----> 9\u001b[1;33m                                          list(map(nltk.word_tokenize, OriginalTweet)),df.OriginalTweet))# tokenize sentences\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     df['tokenized_sentences'] = list(map(lambda OriginalTweet:\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\hp/nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    #df['OriginalTweet'] = df.OriginalTweet.str.lower()\n",
    "    #df['OriginalTweet'] = df.OriginalTweet.str.split('.')  # split texts into individual sentences\n",
    "    df['tokenized_OriginalTweet'] = list(map(lambda OriginalTweet:\n",
    "                                         list(map(nltk.word_tokenize, OriginalTweet)),df.OriginalTweet))# tokenize sentences\n",
    "                                  \n",
    "    df['tokenized_sentences'] = list(map(lambda OriginalTweet:\n",
    "                                         list(map(get_good_tokens, OriginalTweet)),df.tokenized_sentences))  # remove unwanted characters\n",
    "                                         \n",
    "    df['tokenized_sentences'] = list(map(lambda OriginalTweet:\n",
    "                                         list(filter(lambda lst: lst, OriginalTweet)),df.tokenized_sentences))  # remove empty lists\n",
    "                                         \n",
    "\n",
    "w2v_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get transform the documents into sentences for the word2vecmodel\n",
    "# we made a function such that later on when we make the submission, we don't need to write duplicate code\n",
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    df['OriginalTweet'] = df.OriginalTweet.str.lower()\n",
    "    #df['document_sentences'] = df.text.str.split('.')  # split texts into individual sentences\n",
    "    \n",
    "\n",
    "w2v_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OriginalTweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-8468d5d46796>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_frequency_barplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOriginalTweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word Frequencies\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OriginalTweet' is not defined"
     ]
    }
   ],
   "source": [
    "# I made a function out of this since I will use it again later on \n",
    "def word_frequency_barplot(df, nr_top_words=50):\n",
    "    \"\"\" df should have a column named count.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "\n",
    "    sns.barplot(list(range(nr_top_words)), df['count'].values[:nr_top_words], palette='hls', ax=ax)\n",
    "\n",
    "    ax.set_xticks(list(range(nr_top_words)))\n",
    "    ax.set_xticklabels(df.index[:nr_top_words], fontsize=14, rotation=90)\n",
    "    return ax\n",
    "    \n",
    "ax = word_frequency_barplot()\n",
    "ax.set_title(\"Word Frequencies\", fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int32' object has no attribute 'texts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-85d06fd9c3f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#ax.set_xticklabels(OriginalTweet_processed.index, fontsize=16)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOriginalTweet_processed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_height\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int32' object has no attribute 'texts'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFsCAYAAAAHcSrCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQfElEQVR4nO3dX2jdZ/3A8c8Js9XYQkY8J4FdTFGZOhMnXhiK5EI00bRU2xVsFYM4gtNJMMh0rGUVpLaOQRT1wsJwiIm0iGbNhWmmwwtJQdqLOerGLKNI53py2shsZirpzvld/c6P0HUnafPn8+t5va769HkSPjfh3e+Tcr6FWq1WCwAglZaNHgAAuJ5AA0BCAg0ACQk0ACQk0ACQkEADQELLCvT8/Hzs2LEjLly4cN3eCy+8ELt3747+/v7Yv39/XLt2bdWHBIBm0zDQzz33XOzbty/Onz//pvsPP/xwPPbYY3Hy5Mmo1Wpx/Pjx1Z4RAJpOw0AfP348Dh48GKVS6bq9V155Ja5evRr33XdfRETs3r07pqamVn9KAGgydzQ6cOjQoRvuzc7ORrFYrK+LxWKUy+XVmQwAmtgt/SexarUahUKhvq7VakvWAMDNafgE/VY6OzujUqnU15cuXXrTq/BG/vWv16Na9ZHgsJ7a27fE5cvzGz0GNJWWlkLceec7l3X2lgJ91113xebNm+PMmTPxsY99LJ5++uno7e1d8fepVmsCDRvAzx3kdVNX3ENDQ/H8889HRMQTTzwRhw8fjs985jPxn//8JwYHB1d1QABoRoUMr5u8fHnev+RhnRWLW6NSubLRY0BTaWkpRHv7luWdXeNZAICbINAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJLSvQk5OTMTAwEH19fTE2Nnbd/tmzZ+P++++PnTt3xte+9rX497//veqDAkAzaRjocrkco6OjMT4+HhMTE3Hs2LE4d+7ckjOHDh2K4eHhOHHiRLznPe+JJ598cs0GBoBm0DDQMzMz0dPTE21tbdHa2hr9/f0xNTW15Ey1Wo3XX389IiIWFhbi7W9/+9pMCwBNomGgZ2dno1gs1telUinK5fKSM4888kgcOHAgPvGJT8TMzEzs3bt39ScFgCZyR6MD1Wo1CoVCfV2r1Zasr169Gvv374+nnnoquru74xe/+EV897vfjaNHjy57iPb2LSscG1gNxeLWjR4BuIGGge7s7IzTp0/X15VKJUqlUn390ksvxebNm6O7uzsiIr7whS/Ej3/84xUNcfnyfFSrtRV9DXBrisWtUalc2egxoKm0tBSW/VDa8Ip727ZtcerUqZibm4uFhYWYnp6O3t7e+v7dd98dFy9ejJdffjkiIv74xz9GV1fXTY4OAEQs4wm6o6MjRkZGYnBwMBYXF2PPnj3R3d0dQ0NDMTw8HF1dXXH48OH41re+FbVaLdrb2+MHP/jBeswOALetQq1W2/C7ZVfcsP5cccP6W9UrbgBg/Qk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAktK9CTk5MxMDAQfX19MTY2dt3+yy+/HF/+8pdj586d8cADD8Rrr7226oMCQDNpGOhyuRyjo6MxPj4eExMTcezYsTh37lx9v1arxde//vUYGhqKEydOxAc/+ME4evTomg4NALe7hoGemZmJnp6eaGtri9bW1ujv74+pqan6/tmzZ6O1tTV6e3sjIuLBBx+ML33pS2s3MQA0gYaBnp2djWKxWF+XSqUol8v19T/+8Y9417veFY8++mjs2rUrDh48GK2trWszLQA0iTsaHahWq1EoFOrrWq22ZH3t2rX4y1/+Er/61a+iq6srfvSjH8WRI0fiyJEjyx6ivX3LCscGVkOxuHWjRwBuoGGgOzs74/Tp0/V1pVKJUqlUXxeLxbj77rujq6srIiJ27NgRw8PDKxri8uX5qFZrK/oa4NYUi1ujUrmy0WNAU2lpKSz7obThFfe2bdvi1KlTMTc3FwsLCzE9PV3/fXNExEc/+tGYm5uLF198MSIinn322bj33ntvcnQAIGIZT9AdHR0xMjISg4ODsbi4GHv27Inu7u4YGhqK4eHh6Orqip/97Gdx4MCBWFhYiM7Oznj88cfXY3YAuG0VarXaht8tu+KG9eeKG9bfql5xAwDrT6ABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASWlagJycnY2BgIPr6+mJsbOyG5/70pz/FJz/5yVUbDgCa1R2NDpTL5RgdHY3f/va3sWnTpti7d298/OMfj/e9731Lzl26dCl++MMfrtmgANBMGj5Bz8zMRE9PT7S1tUVra2v09/fH1NTUdecOHDgQ3/zmN9dkSABoNg0DPTs7G8Visb4ulUpRLpeXnPnlL38ZH/rQh+IjH/nI6k8IAE2o4RV3tVqNQqFQX9dqtSXrl156Kaanp+Opp56Kixcv3tQQ7e1bburrgFtTLG7d6BGAG2gY6M7Ozjh9+nR9XalUolQq1ddTU1NRqVTi/vvvj8XFxZidnY0vfvGLMT4+vuwhLl+ej2q1tsLRgVtRLG6NSuXKRo8BTaWlpbDsh9JCrVZ7yzKWy+XYt29f/OY3v4l3vOMdsXfv3vj+978f3d3d1529cOFCDA4OxrPPPruigQUa1p9Aw/pbSaAb/g66o6MjRkZGYnBwMD7/+c/Hjh07oru7O4aGhuL555+/5WEBgOs1fIJeD56gYf15gob1t6pP0ADA+hNoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhAQaABISaABISKABICGBBoCEBBoAEhJoAEhIoAEgIYEGgIQEGgASEmgASEigASAhgQaAhJYV6MnJyRgYGIi+vr4YGxu7bv8Pf/hDfO5zn4udO3fGN77xjXjttddWfVAAaCYNA10ul2N0dDTGx8djYmIijh07FufOnavvz8/Px/e+9704evRonDhxIu655574yU9+sqZDA8DtrmGgZ2ZmoqenJ9ra2qK1tTX6+/tjamqqvr+4uBgHDx6Mjo6OiIi455574tVXX127iQGgCTQM9OzsbBSLxfq6VCpFuVyur++888749Kc/HRERV69ejaNHj8anPvWpNRgVAJrHHY0OVKvVKBQK9XWtVluy/l9XrlyJhx56KD7wgQ/Erl27VjREe/uWFZ0HVkexuHWjRwBuoGGgOzs74/Tp0/V1pVKJUqm05Mzs7Gw88MAD0dPTE48++uiKh7h8eT6q1dqKvw64ecXi1qhUrmz0GNBUWloKy34obXjFvW3btjh16lTMzc3FwsJCTE9PR29vb33/jTfeiAcffDA++9nPxv79+9/06RoAWJmGT9AdHR0xMjISg4ODsbi4GHv27Inu7u4YGhqK4eHhuHjxYvztb3+LN954I06ePBkRER/+8Ifj0KFDaz48ANyuCrVabcPvll1xw/pzxQ3rb1WvuAGA9SfQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQkEADQEICDQAJCTQAJCTQAJCQQANAQgINAAkJNAAkJNAAkJBAA0BCAg0ACS0r0JOTkzEwMBB9fX0xNjZ23f4LL7wQu3fvjv7+/ti/f39cu3Zt1QcFgGbSMNDlcjlGR0djfHw8JiYm4tixY3Hu3LklZx5++OF47LHH4uTJk1Gr1eL48eNrNjAANIOGgZ6ZmYmenp5oa2uL1tbW6O/vj6mpqfr+K6+8ElevXo377rsvIiJ27969ZB8AWLk7Gh2YnZ2NYrFYX5dKpfjrX/96w/1isRjlcnlFQ7S0FFZ0HlgdfvZgfa3kZ65hoKvVahQK//cNa7XaknWj/eW48853rug8sDra27ds9AjADTS84u7s7IxKpVJfVyqVKJVKN9y/dOnSkn0AYOUaBnrbtm1x6tSpmJubi4WFhZieno7e3t76/l133RWbN2+OM2fORETE008/vWQfAFi5Qq1WqzU6NDk5GT//+c9jcXEx9uzZE0NDQzE0NBTDw8PR1dUVL774Yhw4cCDm5+fj3nvvjcOHD8emTZvWY34AuC0tK9AAwPrySWIAkJBAA0BCAg0ACQk0ACQk0ACQ0IYGutFbsoC1MT8/Hzt27IgLFy5s9CjQVH7605/G9u3bY/v27fH444+/5dkNC/Ry3pIFrL7nnnsu9u3bF+fPn9/oUaCpzMzMxJ///Of43e9+FxMTE3H27Nl45plnbnh+wwLd6C1ZwNo4fvx4HDx40EfywjorFovxyCOPxKZNm+Jtb3tbvPe9741//vOfNzzf8GUZa6XRW7KAtXHo0KGNHgGa0vvf//76n8+fPx+///3v49e//vUNz2/YE/RqvAULAP6/+fvf/x5f/epX4zvf+U68+93vvuG5DQt0o7dkAcDt5syZM/GVr3wlvv3tb8euXbve8uyGBbrRW7IA4Hby6quvxkMPPRRPPPFEbN++veH5DfsddEdHR4yMjMTg4GD9LVnd3d0bNQ4ArKknn3wy/vvf/8aRI0fqf7d3797Yt2/fm573NisASMgniQFAQgINAAkJNAAkJNAAkJBAA0BCAg0ACQk0ACQk0ACQ0P8ArxDVIRuYkAcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "OriginalTweet_processed = train_data.OriginalTweet.count()\n",
    "\n",
    "#ax.bar(range(3), OriginalTweet)\n",
    "ax.set_xticks(range(3))\n",
    "#ax.set_xticklabels(OriginalTweet_processed.index, fontsize=16)\n",
    "\n",
    "for rect, c, value in zip(ax.patches, ['b', 'r', 'g'], OriginalTweet_processed.texts):\n",
    "    rect.set_color(c)\n",
    "    height = rect.get_height()\n",
    "    width = rect.get_width()\n",
    "    x_loc = rect.get_x()\n",
    "    ax.text(x_loc + width/2, 0.9*height, value, ha='center', va='center', fontsize=18, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
