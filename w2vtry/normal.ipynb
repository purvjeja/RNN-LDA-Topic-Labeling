{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "# model imports\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set() \n",
    "\n",
    "#Tokenizing: converting a document to its atomic elements\n",
    "#Stopping: removing meaningless words\n",
    "#Stemming: merging words that are equivalent in meaning\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')#Regular-Expression Tokenizers -> splits a string into substrings using a regular expression\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('english')#meaningless words in english eg: is, are, the\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()#removing similar objects\n",
    "\n",
    "doc_set = list()\n",
    "#sample documents\n",
    "files = glob.glob('C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv')\n",
    "for file_name in files:\n",
    "    readall = open(file_name,'r')\n",
    "    doc_set.append(readall.read())\n",
    "papers = pd.read_csv('C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv')\n",
    "# Print head\n",
    "papers.head()    \n",
    "print(papers)\n",
    "#doc_a = \"C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv\"\n",
    "#doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "#doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "#doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "#doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [papers]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "import re\n",
    "papers['OriginalTweet_processed'] = \\\n",
    "papers['OriginalTweet'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "papers['OriginalTweet_processed'] = \\\n",
    "papers['OriginalTweet_processed'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "papers['OriginalTweet_processed'].head()\n",
    "# loop through document list\n",
    "for i in papers['OriginalTweet_processed']:# clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    print ('raw\\n%s'%raw)\n",
    "    tokens = tokenizer.tokenize(raw)# remove stop words from tokens\n",
    "    print ('tokens\\n%s'%tokens)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]# stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]# add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "print ('tokens = %s \\n'%tokens)\n",
    "print ('stopped_tokens = %s \\n'%stopped_tokens)\n",
    "print ('stemmed_tokens = %s \\n'%stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)\n",
    "print (len(dictionary))\n",
    "for j in  dictionary:\n",
    "    print (dictionary[j])\n",
    "print (dictionary)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print ('\\n')\n",
    "print (corpus)\n",
    "print ('\\n')\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=len(dictionary)))\n",
    "dictionary = Dictionary(documents=papers.stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2a4d3c75905d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                          \u001b[1;31m#df.tokenized_sentences))  # remove empty lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mw2v_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'OriginalTweet_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-2a4d3c75905d>\u001b[0m in \u001b[0;36mw2v_preprocessing\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnothing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \"\"\"\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#df['document_sentences'] = df.text.str.split('.')  # split texts into individual sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#df['stemmed_tokens'] = list(map(lambda sentences:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    df['tokens'] = df.tokens.str.lower()\n",
    "    #df['document_sentences'] = df.text.str.split('.')  # split texts into individual sentences\n",
    "    #df['stemmed_tokens'] = list(map(lambda sentences:\n",
    "                                         #list(map(nltk.word_tokenize, sentences)),\n",
    "                                         #df.stemmed_tokens))  # tokenize sentences\n",
    "    #df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         #list(map(get_good_tokens, sentences)),\n",
    "                                         #df.tokenized_sentences))  # remove unwanted characters\n",
    "    #df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         #list(filter(lambda lst: lst, sentences)),\n",
    "                                         #df.tokenized_sentences))  # remove empty lists\n",
    "\n",
    "w2v_preprocessing(papers['OriginalTweet_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
