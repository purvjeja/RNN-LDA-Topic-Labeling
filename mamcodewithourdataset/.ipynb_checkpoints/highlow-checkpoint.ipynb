{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6937f81b62e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mreadall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mpapers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;31m# Print head\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Tokenizing: converting a document to its atomic elements\n",
    "#Stopping: removing meaningless words\n",
    "#Stemming: merging words that are equivalent in meaning\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')#Regular-Expression Tokenizers -> splits a string into substrings using a regular expression\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('english')#meaningless words in english eg: is, are, the\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()#removing similar objects\n",
    "\n",
    "doc_set = list()\n",
    "#sample documents\n",
    "files = glob.glob('*C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv')\n",
    "for file_name in files:\n",
    "    readall = open(file_name,'r')\n",
    "    doc_set.append(readall.read())\n",
    "papers = pd.read_csv('C:/Users/hp/Desktop/finalyear/RNN-LDA-Topic-Labeling-main/Initiation-practice/CSV/T.csv')\n",
    "# Print head\n",
    "papers.head()    \n",
    "print(papers)\n",
    "#doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "#doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "#doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "#doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "#doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [papers]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "import re\n",
    "papers['OriginalTweet_processed'] = \\\n",
    "papers['OriginalTweet'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "# Convert the titles to lowercase\n",
    "papers['OriginalTweet_processed'] = \\\n",
    "papers['OriginalTweet_processed'].map(lambda x: x.lower())\n",
    "# Print out the first rows of papers\n",
    "papers['OriginalTweet_processed'].head()\n",
    "# loop through document list\n",
    "\n",
    "for i in papers['OriginalTweet_processed']:# clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    print ('raw\\n%s'%raw)\n",
    "    tokens = tokenizer.tokenize(raw)# remove stop words from tokens\n",
    "    print ('tokens\\n%s'%tokens)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]# stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]# add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "print ('tokens = %s \\n'%tokens)\n",
    "print ('stopped_tokens = %s \\n'%stopped_tokens)\n",
    "print ('stemmed_tokens = %s \\n'%stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)\n",
    "print (len(dictionary))\n",
    "for j in  dictionary:\n",
    "    print (dictionary[j])\n",
    "print (dictionary)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print ('\\n')\n",
    "print (corpus)\n",
    "print ('\\n')\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=len(dictionary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
