{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')#Regular-Expression Tokenizers -> splits a string into substrings using a regular expression\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('english')#meaningless words in english eg: is, are, the\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()#removing similar objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc_set = list()\n",
    "#sample documents\n",
    "files = glob.glob('*.txt')\n",
    "for file_name in files:\n",
    "    readall = open(file_name,'r')\n",
    "    doc_set.append(readall.read())\n",
    "print(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw\n",
      "brocolli is good to eat. my brother likes to eat good brocolli, but not my mother.\n",
      "tokens\n",
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n",
      "raw\n",
      "my mother spends a lot of time driving my brother around to baseball practice.\n",
      "tokens\n",
      "['my', 'mother', 'spends', 'a', 'lot', 'of', 'time', 'driving', 'my', 'brother', 'around', 'to', 'baseball', 'practice']\n",
      "raw\n",
      "some health experts suggest that driving may cause increased tension and blood pressure.\n",
      "tokens\n",
      "['some', 'health', 'experts', 'suggest', 'that', 'driving', 'may', 'cause', 'increased', 'tension', 'and', 'blood', 'pressure']\n",
      "raw\n",
      "i often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\n",
      "tokens\n",
      "['i', 'often', 'feel', 'pressure', 'to', 'perform', 'well', 'at', 'school', 'but', 'my', 'mother', 'never', 'seems', 'to', 'drive', 'my', 'brother', 'to', 'do', 'better']\n",
      "raw\n",
      "health professionals say that brocolli is good for your health.\n",
      "tokens\n",
      "['health', 'professionals', 'say', 'that', 'brocolli', 'is', 'good', 'for', 'your', 'health']\n",
      "tokens = ['health', 'professionals', 'say', 'that', 'brocolli', 'is', 'good', 'for', 'your', 'health'] \n",
      "\n",
      "stopped_tokens = ['health', 'professionals', 'say', 'brocolli', 'good', 'health'] \n",
      "\n",
      "stemmed_tokens = ['health', 'profession', 'say', 'brocolli', 'good', 'health'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:# clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    print ('raw\\n%s'%raw)\n",
    "    tokens = tokenizer.tokenize(raw)# remove stop words from tokens\n",
    "    print ('tokens\\n%s'%tokens)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]# stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]# add tokens to list\n",
    "    texts.append(stopped_tokens)\n",
    "print ('tokens = %s \\n'%tokens)\n",
    "print ('stopped_tokens = %s \\n'%stopped_tokens)\n",
    "print ('stemmed_tokens = %s \\n'%stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brocolli': 0, 'brother': 1, 'eat': 2, 'good': 3, 'likes': 4, 'mother': 5, 'around': 6, 'baseball': 7, 'driving': 8, 'lot': 9, 'practice': 10, 'spends': 11, 'time': 12, 'blood': 13, 'cause': 14, 'experts': 15, 'health': 16, 'increased': 17, 'may': 18, 'pressure': 19, 'suggest': 20, 'tension': 21, 'better': 22, 'drive': 23, 'feel': 24, 'never': 25, 'often': 26, 'perform': 27, 'school': 28, 'seems': 29, 'well': 30, 'professionals': 31, 'say': 32}\n",
      "33\n",
      "brocolli\n",
      "brother\n",
      "eat\n",
      "good\n",
      "likes\n",
      "mother\n",
      "around\n",
      "baseball\n",
      "driving\n",
      "lot\n",
      "practice\n",
      "spends\n",
      "time\n",
      "blood\n",
      "cause\n",
      "experts\n",
      "health\n",
      "increased\n",
      "may\n",
      "pressure\n",
      "suggest\n",
      "tension\n",
      "better\n",
      "drive\n",
      "feel\n",
      "never\n",
      "often\n",
      "perform\n",
      "school\n",
      "seems\n",
      "well\n",
      "professionals\n",
      "say\n",
      "\n",
      "\n",
      "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)], [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(1, 1), (5, 1), (19, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)], [(0, 1), (3, 1), (16, 2), (31, 1), (32, 1)]]\n",
      "\n",
      "\n",
      "[(0, '0.051*\"brocolli\" + 0.051*\"mother\" + 0.051*\"good\" + 0.051*\"health\" + 0.051*\"brother\" + 0.038*\"driving\" + 0.038*\"eat\" + 0.038*\"pressure\" + 0.025*\"likes\" + 0.025*\"well\" + 0.025*\"seems\" + 0.025*\"school\" + 0.025*\"perform\" + 0.025*\"often\" + 0.025*\"never\" + 0.025*\"feel\" + 0.025*\"drive\" + 0.025*\"better\" + 0.025*\"tension\" + 0.025*\"suggest\" + 0.025*\"may\" + 0.025*\"professionals\" + 0.025*\"experts\" + 0.025*\"cause\" + 0.025*\"blood\" + 0.025*\"time\" + 0.025*\"spends\" + 0.025*\"practice\" + 0.025*\"lot\" + 0.025*\"baseball\" + 0.025*\"around\" + 0.025*\"increased\" + 0.025*\"say\"')]\n"
     ]
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)\n",
    "print (len(dictionary))\n",
    "for j in  dictionary:\n",
    "    print (dictionary[j])\n",
    "#print (dictionary)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print ('\\n')\n",
    "print (corpus)\n",
    "print ('\\n')\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=20)\n",
    "print(ldamodel.print_topics(num_topics=1, num_words=len(dictionary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
